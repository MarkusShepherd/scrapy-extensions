{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scrapy Extensions","text":"<p>Documentation: https://MarkusShepherd.github.io/scrapy-extensions</p> <p>Source Code: https://github.com/MarkusShepherd/scrapy-extensions</p> <p>PyPI: https://pypi.org/project/scrapy-extensions/</p> <p>A collection of Scrapy extensions and other utilities</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install scrapy-extensions\n</code></pre>"},{"location":"#development","title":"Development","text":"<ul> <li>Clone this repository</li> <li>Requirements:</li> <li>Poetry</li> <li>Python 3.8+</li> <li>Create a virtual environment and install the dependencies</li> </ul> <pre><code>poetry install\n</code></pre> <ul> <li>Activate the virtual environment</li> </ul> <pre><code>poetry shell\n</code></pre>"},{"location":"#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation is automatically generated from the content of the docs directory and from the docstrings  of the public signatures of the source code. The documentation is updated and published as a Github Pages page automatically as part each release.</p>"},{"location":"#releasing","title":"Releasing","text":"<p>Trigger the Draft release workflow (press Run workflow). This will update the changelog &amp; version and create a GitHub release which is in Draft state.</p> <p>Find the draft release from the GitHub releases and publish it. When  a release is published, it'll trigger release workflow which creates PyPI  release and deploys updated documentation.</p>"},{"location":"#pre-commit","title":"Pre-commit","text":"<p>Pre-commit hooks run all the auto-formatting (<code>ruff format</code>), linters (e.g. <code>ruff</code> and <code>mypy</code>), and other quality  checks to make sure the changeset is in good shape before a commit/push happens.</p> <p>You can install the hooks with (runs for each commit):</p> <pre><code>pre-commit install\n</code></pre> <p>Or if you want them to run only for each push:</p> <pre><code>pre-commit install -t pre-push\n</code></pre> <p>Or if you want e.g. want to run all checks manually for all files:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>This project was generated using the wolt-python-package-cookiecutter template.</p>"},{"location":"api_docs/","title":"API documentation","text":""},{"location":"api_docs/#scrapy_extensions.BlurHashPipeline","title":"<code>BlurHashPipeline</code>","text":"<p>Calculate the BlurHashes of the downloaded images.</p> Source code in <code>src/scrapy_extensions/pipelines.py</code> <pre><code>class BlurHashPipeline:\n    \"\"\"Calculate the BlurHashes of the downloaded images.\"\"\"\n\n    images_store: Path\n    source_field: str\n    target_field: str\n    x_components: int\n    y_components: int\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -&gt; BlurHashPipeline:\n        \"\"\"Init from crawler.\"\"\"\n\n        images_store = crawler.settings.get(\"IMAGES_STORE\")\n        source_field = crawler.settings.get(\"IMAGES_RESULT_FIELD\")\n        target_field = crawler.settings.get(\"BLURHASH_FIELD\")\n\n        if not images_store or not source_field or not target_field:\n            raise NotConfigured\n\n        if not find_spec(\"scrapy_extensions.utils\", \"calculate_blurhash\"):\n            LOGGER.error(\n                \"Unable to import libraries required for BlurHash, \"\n                \"install with `blurhash` option\",\n            )\n            raise NotConfigured\n\n        x_components = crawler.settings.getint(\"BLURHASH_X_COMPONENTS\", 4)\n        y_components = crawler.settings.getint(\"BLURHASH_Y_COMPONENTS\", 4)\n\n        return cls(\n            images_store=images_store,\n            source_field=source_field,\n            target_field=target_field,\n            x_components=x_components,\n            y_components=y_components,\n        )\n\n    def __init__(\n        self,\n        *,\n        images_store: str | Path,\n        source_field: str,\n        target_field: str,\n        x_components: int = 4,\n        y_components: int = 4,\n    ) -&gt; None:\n        self.images_store = Path(images_store).resolve()\n        self.source_field = source_field\n        self.target_field = target_field\n        self.x_components = x_components\n        self.y_components = y_components\n\n    def process_image_obj(\n        self,\n        image_obj: dict[str, Any],\n        x_components: int = 4,\n        y_components: int = 4,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Calculate the BlurHash of a given image.\"\"\"\n\n        image_path = image_obj.get(\"path\")\n        if not image_path:\n            return image_obj\n\n        image_full_path = (self.images_store / image_path).resolve()\n        if not image_full_path or not image_full_path.is_file():\n            LOGGER.warning(\"Unable to locate image file &lt;%s&gt;\", image_full_path)\n            return image_obj\n\n        # Don't modify the original object\n        image_obj = image_obj.copy()\n\n        image_obj[\"blurhash\"] = _calculate_blurhash(\n            path=image_full_path,\n            x_components=x_components,\n            y_components=y_components,\n        )\n\n        return image_obj\n\n    def process_item(self, item: Any, spider: Spider) -&gt; Any:  # noqa: ARG002\n        \"\"\"Calculate the BlurHashes of the downloaded images.\"\"\"\n\n        adapter = ItemAdapter(item)\n\n        image_objs = tuple(arg_to_iter(adapter.get(self.source_field)))\n        if not image_objs:\n            return item\n\n        try:\n            adapter[self.target_field] = [\n                self.process_image_obj(image_obj) for image_obj in image_objs\n            ]\n        except Exception:\n            LOGGER.exception(\"Unable to add field &lt;%s&gt; to the item\", self.target_field)\n\n        return item\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.BlurHashPipeline.from_crawler","title":"<code>from_crawler(crawler: Crawler) -&gt; BlurHashPipeline</code>  <code>classmethod</code>","text":"<p>Init from crawler.</p> Source code in <code>src/scrapy_extensions/pipelines.py</code> <pre><code>@classmethod\ndef from_crawler(cls, crawler: Crawler) -&gt; BlurHashPipeline:\n    \"\"\"Init from crawler.\"\"\"\n\n    images_store = crawler.settings.get(\"IMAGES_STORE\")\n    source_field = crawler.settings.get(\"IMAGES_RESULT_FIELD\")\n    target_field = crawler.settings.get(\"BLURHASH_FIELD\")\n\n    if not images_store or not source_field or not target_field:\n        raise NotConfigured\n\n    if not find_spec(\"scrapy_extensions.utils\", \"calculate_blurhash\"):\n        LOGGER.error(\n            \"Unable to import libraries required for BlurHash, \"\n            \"install with `blurhash` option\",\n        )\n        raise NotConfigured\n\n    x_components = crawler.settings.getint(\"BLURHASH_X_COMPONENTS\", 4)\n    y_components = crawler.settings.getint(\"BLURHASH_Y_COMPONENTS\", 4)\n\n    return cls(\n        images_store=images_store,\n        source_field=source_field,\n        target_field=target_field,\n        x_components=x_components,\n        y_components=y_components,\n    )\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.BlurHashPipeline.process_image_obj","title":"<code>process_image_obj(image_obj: dict[str, Any], x_components: int = 4, y_components: int = 4) -&gt; dict[str, Any]</code>","text":"<p>Calculate the BlurHash of a given image.</p> Source code in <code>src/scrapy_extensions/pipelines.py</code> <pre><code>def process_image_obj(\n    self,\n    image_obj: dict[str, Any],\n    x_components: int = 4,\n    y_components: int = 4,\n) -&gt; dict[str, Any]:\n    \"\"\"Calculate the BlurHash of a given image.\"\"\"\n\n    image_path = image_obj.get(\"path\")\n    if not image_path:\n        return image_obj\n\n    image_full_path = (self.images_store / image_path).resolve()\n    if not image_full_path or not image_full_path.is_file():\n        LOGGER.warning(\"Unable to locate image file &lt;%s&gt;\", image_full_path)\n        return image_obj\n\n    # Don't modify the original object\n    image_obj = image_obj.copy()\n\n    image_obj[\"blurhash\"] = _calculate_blurhash(\n        path=image_full_path,\n        x_components=x_components,\n        y_components=y_components,\n    )\n\n    return image_obj\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.BlurHashPipeline.process_item","title":"<code>process_item(item: Any, spider: Spider) -&gt; Any</code>","text":"<p>Calculate the BlurHashes of the downloaded images.</p> Source code in <code>src/scrapy_extensions/pipelines.py</code> <pre><code>def process_item(self, item: Any, spider: Spider) -&gt; Any:  # noqa: ARG002\n    \"\"\"Calculate the BlurHashes of the downloaded images.\"\"\"\n\n    adapter = ItemAdapter(item)\n\n    image_objs = tuple(arg_to_iter(adapter.get(self.source_field)))\n    if not image_objs:\n        return item\n\n    try:\n        adapter[self.target_field] = [\n            self.process_image_obj(image_obj) for image_obj in image_objs\n        ]\n    except Exception:\n        LOGGER.exception(\"Unable to add field &lt;%s&gt; to the item\", self.target_field)\n\n    return item\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.DelayedRetryMiddleware","title":"<code>DelayedRetryMiddleware</code>","text":"<p>               Bases: <code>RetryMiddleware</code></p> <p>retry requests with a delay (async/await version)</p>"},{"location":"api_docs/#scrapy_extensions.DelayedRetryMiddleware--notes","title":"Notes","text":"<ul> <li>Uses <code>asyncio.sleep</code> to implement the delay.</li> <li><code>process_response</code> is an async coroutine; Scrapy accepts coroutines from   middleware methods and will await them appropriately when using an   asyncio-compatible reactor.</li> <li>Behaviour and configuration keys are kept compatible with the original   implementation.</li> </ul> Source code in <code>src/scrapy_extensions/downloadermiddlewares.py</code> <pre><code>class DelayedRetryMiddleware(RetryMiddleware):\n    \"\"\"retry requests with a delay (async/await version)\n\n    Notes\n    -----\n    - Uses `asyncio.sleep` to implement the delay.\n    - `process_response` is an async coroutine; Scrapy accepts coroutines from\n      middleware methods and will await them appropriately when using an\n      asyncio-compatible reactor.\n    - Behaviour and configuration keys are kept compatible with the original\n      implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        settings: Settings,\n    ):\n        super().__init__(settings)\n\n        delayed_retry_http_codes_settings = settings.getlist(\"DELAYED_RETRY_HTTP_CODES\")\n        try:\n            delayed_retry_http_codes = (\n                int(http_code) for http_code in delayed_retry_http_codes_settings\n            )\n        except ValueError as exc:\n            LOGGER.exception(\n                \"Invalid http code(s) in DELAYED_RETRY_HTTP_CODES: %s\",\n                delayed_retry_http_codes_settings,\n            )\n            raise NotConfigured from exc\n        self.delayed_retry_http_codes = frozenset(\n            filter(None, delayed_retry_http_codes),\n        )\n\n        self.delayed_retry_max_retry_times = settings.getint(\"DELAYED_RETRY_TIMES\", -1)\n        self.delayed_retry_priority_adjust = settings.getint(\n            \"DELAYED_RETRY_PRIORITY_ADJUST\",\n            self.priority_adjust,\n        )\n        self.delayed_retry_delay = settings.getfloat(\"DELAYED_RETRY_DELAY\", 1)\n        self.delayed_retry_backoff = settings.getbool(\"DELAYED_RETRY_BACKOFF\")\n        self.delayed_retry_backoff_max_delay = settings.getfloat(\n            \"DELAYED_RETRY_BACKOFF_MAX_DELAY\",\n            10 * self.delayed_retry_delay,\n        )\n\n    async def process_response(  # type: ignore[override]\n        self,\n        request: Request,\n        response: Response,\n        spider: Spider,\n    ) -&gt; Request | Response:\n        \"\"\"retry certain requests with delay\n\n        This method is now a coroutine. If the response status matches a\n        delayed-retry code, we await the computed delay and then return the\n        retry Request (or None, in which case the original response is\n        returned). Otherwise we delegate to the parent implementation.\n        \"\"\"\n\n        if request.meta.get(\"dont_retry\"):\n            return response\n\n        if response.status in self.delayed_retry_http_codes:\n            reason = response_status_message(response.status)\n            req = await self._delayed_retry(request, reason, spider)\n            return req or response\n\n        # Delegate to parent. The parent may return a value or a Deferred/coroutine.\n        parent_result = super().process_response(request, response, spider)\n        if asyncio.iscoroutine(parent_result):\n            return await parent_result  # type: ignore[no-any-return]\n        return parent_result\n\n    async def _delayed_retry(\n        self,\n        request: Request,\n        reason: str,\n        spider: Spider,\n    ) -&gt; Request | None:\n        \"\"\"Compute retry Request and await the configured delay before returning it.\"\"\"\n\n        max_retry_times = request.meta.get(\n            \"max_retry_times\",\n            self.delayed_retry_max_retry_times,\n        )\n        if max_retry_times &lt; 0:\n            max_retry_times = sys.maxsize\n        priority_adjust = request.meta.get(\n            \"priority_adjust\",\n            self.delayed_retry_priority_adjust,\n        )\n\n        req = get_retry_request(\n            request=request,\n            spider=spider,\n            reason=reason,\n            max_retry_times=max_retry_times,\n            priority_adjust=priority_adjust,\n        )\n\n        if req is None:\n            return None\n\n        delay = request.meta.get(\"retry_delay\", self.delayed_retry_delay)\n        req.meta[\"retry_delay\"] = (\n            min(2 * delay, self.delayed_retry_backoff_max_delay)\n            if self.delayed_retry_backoff\n            else delay\n        )\n\n        LOGGER.debug(\"Retry request %r in %.1f second(s)\", req, delay)\n\n        # Non-blocking sleep \u2014 preserves reactor responsiveness in asyncio mode.\n        await asyncio.sleep(delay)\n        return req\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.DelayedRetryMiddleware.process_response","title":"<code>process_response(request: Request, response: Response, spider: Spider) -&gt; Request | Response</code>  <code>async</code>","text":"<p>retry certain requests with delay</p> <p>This method is now a coroutine. If the response status matches a delayed-retry code, we await the computed delay and then return the retry Request (or None, in which case the original response is returned). Otherwise we delegate to the parent implementation.</p> Source code in <code>src/scrapy_extensions/downloadermiddlewares.py</code> <pre><code>async def process_response(  # type: ignore[override]\n    self,\n    request: Request,\n    response: Response,\n    spider: Spider,\n) -&gt; Request | Response:\n    \"\"\"retry certain requests with delay\n\n    This method is now a coroutine. If the response status matches a\n    delayed-retry code, we await the computed delay and then return the\n    retry Request (or None, in which case the original response is\n    returned). Otherwise we delegate to the parent implementation.\n    \"\"\"\n\n    if request.meta.get(\"dont_retry\"):\n        return response\n\n    if response.status in self.delayed_retry_http_codes:\n        reason = response_status_message(response.status)\n        req = await self._delayed_retry(request, reason, spider)\n        return req or response\n\n    # Delegate to parent. The parent may return a value or a Deferred/coroutine.\n    parent_result = super().process_response(request, response, spider)\n    if asyncio.iscoroutine(parent_result):\n        return await parent_result  # type: ignore[no-any-return]\n    return parent_result\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.LoopingExtension","title":"<code>LoopingExtension</code>","text":"<p>Run a task in a loop.</p> Source code in <code>src/scrapy_extensions/extensions.py</code> <pre><code>class LoopingExtension:\n    \"\"\"Run a task in a loop.\"\"\"\n\n    task: Callable[..., object]\n    _task: LoopingCall | None = None\n    _interval: float\n\n    def setup_looping_task(\n        self,\n        task: Callable[..., object],\n        crawler: Crawler,\n        interval: float,\n    ) -&gt; None:\n        \"\"\"Setup task to run periodically at a given interval.\"\"\"\n\n        self.task = task\n        self._interval = interval\n        crawler.signals.connect(\n            self._spider_opened,\n            signal=spider_opened,\n        )\n        crawler.signals.connect(\n            self._spider_closed,\n            signal=spider_closed,\n        )\n\n    def _spider_opened(self, spider: Spider) -&gt; None:\n        if self._task is None:\n            self._task = LoopingCall(self.task, spider=spider)\n        self._task.start(self._interval, now=False)\n\n    def _spider_closed(self) -&gt; None:\n        if self._task is None:\n            LOGGER.warning(\"No task was started\")\n            return\n\n        if self._task.running:\n            self._task.stop()\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.LoopingExtension.setup_looping_task","title":"<code>setup_looping_task(task: Callable[..., object], crawler: Crawler, interval: float) -&gt; None</code>","text":"<p>Setup task to run periodically at a given interval.</p> Source code in <code>src/scrapy_extensions/extensions.py</code> <pre><code>def setup_looping_task(\n    self,\n    task: Callable[..., object],\n    crawler: Crawler,\n    interval: float,\n) -&gt; None:\n    \"\"\"Setup task to run periodically at a given interval.\"\"\"\n\n    self.task = task\n    self._interval = interval\n    crawler.signals.connect(\n        self._spider_opened,\n        signal=spider_opened,\n    )\n    crawler.signals.connect(\n        self._spider_closed,\n        signal=spider_closed,\n    )\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.NicerAutoThrottle","title":"<code>NicerAutoThrottle</code>","text":"<p>               Bases: <code>AutoThrottle</code></p> <p>Autothrottling with exponential backoff depending on status codes.</p> Source code in <code>src/scrapy_extensions/extensions.py</code> <pre><code>class NicerAutoThrottle(AutoThrottle):\n    \"\"\"Autothrottling with exponential backoff depending on status codes.\"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -&gt; NicerAutoThrottle:\n        http_codes_settings = crawler.settings.getlist(\"AUTOTHROTTLE_HTTP_CODES\")\n\n        try:\n            http_codes = (\n                int(http_code) for http_code in arg_to_iter(http_codes_settings)\n            )\n\n        except ValueError:\n            LOGGER.exception(\"Invalid HTTP code: %s\", http_codes_settings)\n            http_codes = None\n\n        return cls(crawler, http_codes)\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        http_codes: Iterable[int] | None = None,\n    ):\n        super().__init__(crawler)\n        self.http_codes: frozenset[int] = frozenset(\n            filter(None, arg_to_iter(http_codes)),\n        )\n        LOGGER.info(\"Throttle requests on status codes: %s\", sorted(self.http_codes))\n\n    def _adjust_delay(\n        self,\n        slot: Slot,\n        latency: float,\n        response: Response,\n    ) -&gt; None:\n        super()._adjust_delay(slot, latency, response)\n\n        if response.status not in self.http_codes:\n            return\n\n        new_delay = (\n            min(2 * slot.delay, self.maxdelay) if self.maxdelay else 2 * slot.delay\n        )\n\n        LOGGER.debug(\n            \"Status &lt;%d&gt; throttled from %.1fs to %.1fs: %r\",\n            response.status,\n            slot.delay,\n            new_delay,\n            response,\n        )\n\n        slot.delay = new_delay\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.QuietLogFormatter","title":"<code>QuietLogFormatter</code>","text":"<p>               Bases: <code>LogFormatter</code></p> <p>Be quieter about scraped items.</p> Source code in <code>src/scrapy_extensions/loggers.py</code> <pre><code>class QuietLogFormatter(LogFormatter):\n    \"\"\"Be quieter about scraped items.\"\"\"\n\n    def scraped(  # type: ignore[override]\n        self,\n        item: Any,\n        response: Response,\n        spider: Spider,\n    ) -&gt; LogFormatterResult | None:\n        return (\n            super().scraped(item, response, spider)\n            if spider.settings.getbool(\"LOG_SCRAPED_ITEMS\")\n            else None\n        )\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.downloadermiddlewares","title":"<code>downloadermiddlewares</code>","text":"<p>Scrapy downloader middleware (async/await rewrite)</p> <p>This middleware preserves the same behaviour as the original Deferred-based implementation but uses Python coroutines (<code>async</code>/<code>await</code>) and <code>asyncio.sleep</code> for the delay. The public behaviour (delayed retries, backoff, priority adjust, config keys) is unchanged.</p>"},{"location":"api_docs/#scrapy_extensions.downloadermiddlewares.DelayedRetryMiddleware","title":"<code>DelayedRetryMiddleware</code>","text":"<p>               Bases: <code>RetryMiddleware</code></p> <p>retry requests with a delay (async/await version)</p>"},{"location":"api_docs/#scrapy_extensions.downloadermiddlewares.DelayedRetryMiddleware--notes","title":"Notes","text":"<ul> <li>Uses <code>asyncio.sleep</code> to implement the delay.</li> <li><code>process_response</code> is an async coroutine; Scrapy accepts coroutines from   middleware methods and will await them appropriately when using an   asyncio-compatible reactor.</li> <li>Behaviour and configuration keys are kept compatible with the original   implementation.</li> </ul> Source code in <code>src/scrapy_extensions/downloadermiddlewares.py</code> <pre><code>class DelayedRetryMiddleware(RetryMiddleware):\n    \"\"\"retry requests with a delay (async/await version)\n\n    Notes\n    -----\n    - Uses `asyncio.sleep` to implement the delay.\n    - `process_response` is an async coroutine; Scrapy accepts coroutines from\n      middleware methods and will await them appropriately when using an\n      asyncio-compatible reactor.\n    - Behaviour and configuration keys are kept compatible with the original\n      implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        settings: Settings,\n    ):\n        super().__init__(settings)\n\n        delayed_retry_http_codes_settings = settings.getlist(\"DELAYED_RETRY_HTTP_CODES\")\n        try:\n            delayed_retry_http_codes = (\n                int(http_code) for http_code in delayed_retry_http_codes_settings\n            )\n        except ValueError as exc:\n            LOGGER.exception(\n                \"Invalid http code(s) in DELAYED_RETRY_HTTP_CODES: %s\",\n                delayed_retry_http_codes_settings,\n            )\n            raise NotConfigured from exc\n        self.delayed_retry_http_codes = frozenset(\n            filter(None, delayed_retry_http_codes),\n        )\n\n        self.delayed_retry_max_retry_times = settings.getint(\"DELAYED_RETRY_TIMES\", -1)\n        self.delayed_retry_priority_adjust = settings.getint(\n            \"DELAYED_RETRY_PRIORITY_ADJUST\",\n            self.priority_adjust,\n        )\n        self.delayed_retry_delay = settings.getfloat(\"DELAYED_RETRY_DELAY\", 1)\n        self.delayed_retry_backoff = settings.getbool(\"DELAYED_RETRY_BACKOFF\")\n        self.delayed_retry_backoff_max_delay = settings.getfloat(\n            \"DELAYED_RETRY_BACKOFF_MAX_DELAY\",\n            10 * self.delayed_retry_delay,\n        )\n\n    async def process_response(  # type: ignore[override]\n        self,\n        request: Request,\n        response: Response,\n        spider: Spider,\n    ) -&gt; Request | Response:\n        \"\"\"retry certain requests with delay\n\n        This method is now a coroutine. If the response status matches a\n        delayed-retry code, we await the computed delay and then return the\n        retry Request (or None, in which case the original response is\n        returned). Otherwise we delegate to the parent implementation.\n        \"\"\"\n\n        if request.meta.get(\"dont_retry\"):\n            return response\n\n        if response.status in self.delayed_retry_http_codes:\n            reason = response_status_message(response.status)\n            req = await self._delayed_retry(request, reason, spider)\n            return req or response\n\n        # Delegate to parent. The parent may return a value or a Deferred/coroutine.\n        parent_result = super().process_response(request, response, spider)\n        if asyncio.iscoroutine(parent_result):\n            return await parent_result  # type: ignore[no-any-return]\n        return parent_result\n\n    async def _delayed_retry(\n        self,\n        request: Request,\n        reason: str,\n        spider: Spider,\n    ) -&gt; Request | None:\n        \"\"\"Compute retry Request and await the configured delay before returning it.\"\"\"\n\n        max_retry_times = request.meta.get(\n            \"max_retry_times\",\n            self.delayed_retry_max_retry_times,\n        )\n        if max_retry_times &lt; 0:\n            max_retry_times = sys.maxsize\n        priority_adjust = request.meta.get(\n            \"priority_adjust\",\n            self.delayed_retry_priority_adjust,\n        )\n\n        req = get_retry_request(\n            request=request,\n            spider=spider,\n            reason=reason,\n            max_retry_times=max_retry_times,\n            priority_adjust=priority_adjust,\n        )\n\n        if req is None:\n            return None\n\n        delay = request.meta.get(\"retry_delay\", self.delayed_retry_delay)\n        req.meta[\"retry_delay\"] = (\n            min(2 * delay, self.delayed_retry_backoff_max_delay)\n            if self.delayed_retry_backoff\n            else delay\n        )\n\n        LOGGER.debug(\"Retry request %r in %.1f second(s)\", req, delay)\n\n        # Non-blocking sleep \u2014 preserves reactor responsiveness in asyncio mode.\n        await asyncio.sleep(delay)\n        return req\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.downloadermiddlewares.DelayedRetryMiddleware.process_response","title":"<code>process_response(request: Request, response: Response, spider: Spider) -&gt; Request | Response</code>  <code>async</code>","text":"<p>retry certain requests with delay</p> <p>This method is now a coroutine. If the response status matches a delayed-retry code, we await the computed delay and then return the retry Request (or None, in which case the original response is returned). Otherwise we delegate to the parent implementation.</p> Source code in <code>src/scrapy_extensions/downloadermiddlewares.py</code> <pre><code>async def process_response(  # type: ignore[override]\n    self,\n    request: Request,\n    response: Response,\n    spider: Spider,\n) -&gt; Request | Response:\n    \"\"\"retry certain requests with delay\n\n    This method is now a coroutine. If the response status matches a\n    delayed-retry code, we await the computed delay and then return the\n    retry Request (or None, in which case the original response is\n    returned). Otherwise we delegate to the parent implementation.\n    \"\"\"\n\n    if request.meta.get(\"dont_retry\"):\n        return response\n\n    if response.status in self.delayed_retry_http_codes:\n        reason = response_status_message(response.status)\n        req = await self._delayed_retry(request, reason, spider)\n        return req or response\n\n    # Delegate to parent. The parent may return a value or a Deferred/coroutine.\n    parent_result = super().process_response(request, response, spider)\n    if asyncio.iscoroutine(parent_result):\n        return await parent_result  # type: ignore[no-any-return]\n    return parent_result\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.extensions","title":"<code>extensions</code>","text":"<p>Extensions.</p>"},{"location":"api_docs/#scrapy_extensions.extensions.LoopingExtension","title":"<code>LoopingExtension</code>","text":"<p>Run a task in a loop.</p> Source code in <code>src/scrapy_extensions/extensions.py</code> <pre><code>class LoopingExtension:\n    \"\"\"Run a task in a loop.\"\"\"\n\n    task: Callable[..., object]\n    _task: LoopingCall | None = None\n    _interval: float\n\n    def setup_looping_task(\n        self,\n        task: Callable[..., object],\n        crawler: Crawler,\n        interval: float,\n    ) -&gt; None:\n        \"\"\"Setup task to run periodically at a given interval.\"\"\"\n\n        self.task = task\n        self._interval = interval\n        crawler.signals.connect(\n            self._spider_opened,\n            signal=spider_opened,\n        )\n        crawler.signals.connect(\n            self._spider_closed,\n            signal=spider_closed,\n        )\n\n    def _spider_opened(self, spider: Spider) -&gt; None:\n        if self._task is None:\n            self._task = LoopingCall(self.task, spider=spider)\n        self._task.start(self._interval, now=False)\n\n    def _spider_closed(self) -&gt; None:\n        if self._task is None:\n            LOGGER.warning(\"No task was started\")\n            return\n\n        if self._task.running:\n            self._task.stop()\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.extensions.LoopingExtension.setup_looping_task","title":"<code>setup_looping_task(task: Callable[..., object], crawler: Crawler, interval: float) -&gt; None</code>","text":"<p>Setup task to run periodically at a given interval.</p> Source code in <code>src/scrapy_extensions/extensions.py</code> <pre><code>def setup_looping_task(\n    self,\n    task: Callable[..., object],\n    crawler: Crawler,\n    interval: float,\n) -&gt; None:\n    \"\"\"Setup task to run periodically at a given interval.\"\"\"\n\n    self.task = task\n    self._interval = interval\n    crawler.signals.connect(\n        self._spider_opened,\n        signal=spider_opened,\n    )\n    crawler.signals.connect(\n        self._spider_closed,\n        signal=spider_closed,\n    )\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.extensions.NicerAutoThrottle","title":"<code>NicerAutoThrottle</code>","text":"<p>               Bases: <code>AutoThrottle</code></p> <p>Autothrottling with exponential backoff depending on status codes.</p> Source code in <code>src/scrapy_extensions/extensions.py</code> <pre><code>class NicerAutoThrottle(AutoThrottle):\n    \"\"\"Autothrottling with exponential backoff depending on status codes.\"\"\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -&gt; NicerAutoThrottle:\n        http_codes_settings = crawler.settings.getlist(\"AUTOTHROTTLE_HTTP_CODES\")\n\n        try:\n            http_codes = (\n                int(http_code) for http_code in arg_to_iter(http_codes_settings)\n            )\n\n        except ValueError:\n            LOGGER.exception(\"Invalid HTTP code: %s\", http_codes_settings)\n            http_codes = None\n\n        return cls(crawler, http_codes)\n\n    def __init__(\n        self,\n        crawler: Crawler,\n        http_codes: Iterable[int] | None = None,\n    ):\n        super().__init__(crawler)\n        self.http_codes: frozenset[int] = frozenset(\n            filter(None, arg_to_iter(http_codes)),\n        )\n        LOGGER.info(\"Throttle requests on status codes: %s\", sorted(self.http_codes))\n\n    def _adjust_delay(\n        self,\n        slot: Slot,\n        latency: float,\n        response: Response,\n    ) -&gt; None:\n        super()._adjust_delay(slot, latency, response)\n\n        if response.status not in self.http_codes:\n            return\n\n        new_delay = (\n            min(2 * slot.delay, self.maxdelay) if self.maxdelay else 2 * slot.delay\n        )\n\n        LOGGER.debug(\n            \"Status &lt;%d&gt; throttled from %.1fs to %.1fs: %r\",\n            response.status,\n            slot.delay,\n            new_delay,\n            response,\n        )\n\n        slot.delay = new_delay\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.loggers","title":"<code>loggers</code>","text":"<p>Logging classes.</p>"},{"location":"api_docs/#scrapy_extensions.loggers.QuietLogFormatter","title":"<code>QuietLogFormatter</code>","text":"<p>               Bases: <code>LogFormatter</code></p> <p>Be quieter about scraped items.</p> Source code in <code>src/scrapy_extensions/loggers.py</code> <pre><code>class QuietLogFormatter(LogFormatter):\n    \"\"\"Be quieter about scraped items.\"\"\"\n\n    def scraped(  # type: ignore[override]\n        self,\n        item: Any,\n        response: Response,\n        spider: Spider,\n    ) -&gt; LogFormatterResult | None:\n        return (\n            super().scraped(item, response, spider)\n            if spider.settings.getbool(\"LOG_SCRAPED_ITEMS\")\n            else None\n        )\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.pipelines","title":"<code>pipelines</code>","text":"<p>Scrapy item pipelines</p>"},{"location":"api_docs/#scrapy_extensions.pipelines.BlurHashPipeline","title":"<code>BlurHashPipeline</code>","text":"<p>Calculate the BlurHashes of the downloaded images.</p> Source code in <code>src/scrapy_extensions/pipelines.py</code> <pre><code>class BlurHashPipeline:\n    \"\"\"Calculate the BlurHashes of the downloaded images.\"\"\"\n\n    images_store: Path\n    source_field: str\n    target_field: str\n    x_components: int\n    y_components: int\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler) -&gt; BlurHashPipeline:\n        \"\"\"Init from crawler.\"\"\"\n\n        images_store = crawler.settings.get(\"IMAGES_STORE\")\n        source_field = crawler.settings.get(\"IMAGES_RESULT_FIELD\")\n        target_field = crawler.settings.get(\"BLURHASH_FIELD\")\n\n        if not images_store or not source_field or not target_field:\n            raise NotConfigured\n\n        if not find_spec(\"scrapy_extensions.utils\", \"calculate_blurhash\"):\n            LOGGER.error(\n                \"Unable to import libraries required for BlurHash, \"\n                \"install with `blurhash` option\",\n            )\n            raise NotConfigured\n\n        x_components = crawler.settings.getint(\"BLURHASH_X_COMPONENTS\", 4)\n        y_components = crawler.settings.getint(\"BLURHASH_Y_COMPONENTS\", 4)\n\n        return cls(\n            images_store=images_store,\n            source_field=source_field,\n            target_field=target_field,\n            x_components=x_components,\n            y_components=y_components,\n        )\n\n    def __init__(\n        self,\n        *,\n        images_store: str | Path,\n        source_field: str,\n        target_field: str,\n        x_components: int = 4,\n        y_components: int = 4,\n    ) -&gt; None:\n        self.images_store = Path(images_store).resolve()\n        self.source_field = source_field\n        self.target_field = target_field\n        self.x_components = x_components\n        self.y_components = y_components\n\n    def process_image_obj(\n        self,\n        image_obj: dict[str, Any],\n        x_components: int = 4,\n        y_components: int = 4,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Calculate the BlurHash of a given image.\"\"\"\n\n        image_path = image_obj.get(\"path\")\n        if not image_path:\n            return image_obj\n\n        image_full_path = (self.images_store / image_path).resolve()\n        if not image_full_path or not image_full_path.is_file():\n            LOGGER.warning(\"Unable to locate image file &lt;%s&gt;\", image_full_path)\n            return image_obj\n\n        # Don't modify the original object\n        image_obj = image_obj.copy()\n\n        image_obj[\"blurhash\"] = _calculate_blurhash(\n            path=image_full_path,\n            x_components=x_components,\n            y_components=y_components,\n        )\n\n        return image_obj\n\n    def process_item(self, item: Any, spider: Spider) -&gt; Any:  # noqa: ARG002\n        \"\"\"Calculate the BlurHashes of the downloaded images.\"\"\"\n\n        adapter = ItemAdapter(item)\n\n        image_objs = tuple(arg_to_iter(adapter.get(self.source_field)))\n        if not image_objs:\n            return item\n\n        try:\n            adapter[self.target_field] = [\n                self.process_image_obj(image_obj) for image_obj in image_objs\n            ]\n        except Exception:\n            LOGGER.exception(\"Unable to add field &lt;%s&gt; to the item\", self.target_field)\n\n        return item\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.pipelines.BlurHashPipeline.from_crawler","title":"<code>from_crawler(crawler: Crawler) -&gt; BlurHashPipeline</code>  <code>classmethod</code>","text":"<p>Init from crawler.</p> Source code in <code>src/scrapy_extensions/pipelines.py</code> <pre><code>@classmethod\ndef from_crawler(cls, crawler: Crawler) -&gt; BlurHashPipeline:\n    \"\"\"Init from crawler.\"\"\"\n\n    images_store = crawler.settings.get(\"IMAGES_STORE\")\n    source_field = crawler.settings.get(\"IMAGES_RESULT_FIELD\")\n    target_field = crawler.settings.get(\"BLURHASH_FIELD\")\n\n    if not images_store or not source_field or not target_field:\n        raise NotConfigured\n\n    if not find_spec(\"scrapy_extensions.utils\", \"calculate_blurhash\"):\n        LOGGER.error(\n            \"Unable to import libraries required for BlurHash, \"\n            \"install with `blurhash` option\",\n        )\n        raise NotConfigured\n\n    x_components = crawler.settings.getint(\"BLURHASH_X_COMPONENTS\", 4)\n    y_components = crawler.settings.getint(\"BLURHASH_Y_COMPONENTS\", 4)\n\n    return cls(\n        images_store=images_store,\n        source_field=source_field,\n        target_field=target_field,\n        x_components=x_components,\n        y_components=y_components,\n    )\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.pipelines.BlurHashPipeline.process_image_obj","title":"<code>process_image_obj(image_obj: dict[str, Any], x_components: int = 4, y_components: int = 4) -&gt; dict[str, Any]</code>","text":"<p>Calculate the BlurHash of a given image.</p> Source code in <code>src/scrapy_extensions/pipelines.py</code> <pre><code>def process_image_obj(\n    self,\n    image_obj: dict[str, Any],\n    x_components: int = 4,\n    y_components: int = 4,\n) -&gt; dict[str, Any]:\n    \"\"\"Calculate the BlurHash of a given image.\"\"\"\n\n    image_path = image_obj.get(\"path\")\n    if not image_path:\n        return image_obj\n\n    image_full_path = (self.images_store / image_path).resolve()\n    if not image_full_path or not image_full_path.is_file():\n        LOGGER.warning(\"Unable to locate image file &lt;%s&gt;\", image_full_path)\n        return image_obj\n\n    # Don't modify the original object\n    image_obj = image_obj.copy()\n\n    image_obj[\"blurhash\"] = _calculate_blurhash(\n        path=image_full_path,\n        x_components=x_components,\n        y_components=y_components,\n    )\n\n    return image_obj\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.pipelines.BlurHashPipeline.process_item","title":"<code>process_item(item: Any, spider: Spider) -&gt; Any</code>","text":"<p>Calculate the BlurHashes of the downloaded images.</p> Source code in <code>src/scrapy_extensions/pipelines.py</code> <pre><code>def process_item(self, item: Any, spider: Spider) -&gt; Any:  # noqa: ARG002\n    \"\"\"Calculate the BlurHashes of the downloaded images.\"\"\"\n\n    adapter = ItemAdapter(item)\n\n    image_objs = tuple(arg_to_iter(adapter.get(self.source_field)))\n    if not image_objs:\n        return item\n\n    try:\n        adapter[self.target_field] = [\n            self.process_image_obj(image_obj) for image_obj in image_objs\n        ]\n    except Exception:\n        LOGGER.exception(\"Unable to add field &lt;%s&gt; to the item\", self.target_field)\n\n    return item\n</code></pre>"},{"location":"api_docs/#scrapy_extensions.utils","title":"<code>utils</code>","text":"<p>Utility functions.</p>"},{"location":"api_docs/#scrapy_extensions.utils.calculate_blurhash","title":"<code>calculate_blurhash(image: str | Path | PIL.Image.Image, x_components: int = 4, y_components: int = 4) -&gt; str</code>","text":"<p>Calculate the blurhash of a given image.</p> Source code in <code>src/scrapy_extensions/utils.py</code> <pre><code>def calculate_blurhash(\n    image: str | Path | PIL.Image.Image,\n    x_components: int = 4,\n    y_components: int = 4,\n) -&gt; str:\n    \"\"\"Calculate the blurhash of a given image.\"\"\"\n\n    import numpy as np\n    from blurhash_numba import encode\n    from PIL import Image, ImageOps\n\n    image = image if isinstance(image, Image.Image) else Image.open(image)\n    image = ImageOps.fit(\n        image=image,\n        size=(32 * x_components, 32 * y_components),\n        centering=(0.5, 0),\n    )\n    image_array = np.array(image.convert(\"RGB\"), dtype=float)\n\n    blurhash = encode(\n        image=image_array,\n        x_components=x_components,\n        y_components=y_components,\n    )\n    assert isinstance(blurhash, str)\n    return blurhash\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#110-2025-10-16","title":"1.1.0 - 2025-10-16","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Added support for Py3.14; dropped Py3.9</li> <li>Rewrite <code>DelayedRetryMiddleware</code> to use async instead of Twisted Deferred</li> </ul>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Added <code>AuthHeaderMiddleware</code> from board-game-scraper</li> </ul>"},{"location":"changelog/#102-2024-11-18","title":"1.0.2 - 2024-11-18","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Better imports</li> </ul>"},{"location":"changelog/#101-2024-11-17","title":"1.0.1 - 2024-11-17","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Fixed a bug in <code>DelayedRetryMiddleware</code> which resulted in problems with the Twisted reactor</li> </ul>"},{"location":"changelog/#100-2024-11-17","title":"1.0.0 - 2024-11-17","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release as v1. Ported over the following classes:</li> <li><code>BlurHashPipeline</code></li> <li><code>DelayedRetryMiddleware</code></li> <li><code>LoopingExtension</code></li> <li><code>NicerAutoThrottle</code></li> <li><code>QuietLogFormatter</code></li> </ul>"}]}